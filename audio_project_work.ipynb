{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0c96513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from scipy.io.wavfile import read\n",
    "import wave\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import  Conv1D, GRU, Dropout, Input, Dense, Conv2D, Reshape, MaxPooling2D\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy import linalg\n",
    "from pydub import AudioSegment\n",
    "random.seed(111)\n",
    "fs = 16000\n",
    "\n",
    "#D:\\Koulujutut\\audio_project\\training_data\\car\n",
    "#path = pathlib.Path().resolve()\n",
    "#str(path.parents[0])}\n",
    "\n",
    "car_train_data_path = f\"../Project/audio_project_data/training_data/car/\"\n",
    "tram_train_data_path = f\"../Project/audio_project_data/training_data/tram/\"\n",
    "\n",
    "car_val_data_path = f\"../Project/audio_project_data/validation_data/car/\"\n",
    "tram_val_data_path = f\"../Project/audio_project_data/validation_data/tram/\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfbb41ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Project/audio_project_data/training_data/car/publictransport_car (2).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (15).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (18).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (21).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (6).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (12).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (14).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (7).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (10).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (20).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (16).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (5).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (8).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (17).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (19).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (9).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (11).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (1).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (4).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (13).mp3\n",
      "../Project/audio_project_data/training_data/car/publictransport_car (3).mp3\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(car_train_data_path):\n",
    "        if file.endswith(\".mp3\"):\n",
    "            print(os.path.join(car_train_data_path, file))\n",
    "            audio = AudioSegment.from_mp3(os.path.join(car_train_data_path, file))\n",
    "            wav_file = file.replace(\"mp3\", \"\")\n",
    "            audio.export(os.path.join(car_train_data_path, wav_file), format=\"wav\")\n",
    "\n",
    "for file in os.listdir(tram_train_data_path):\n",
    "    if file.endswith(\".mp3\"):\n",
    "        audio = AudioSegment.from_mp3(os.path.join(tram_train_data_path, file))\n",
    "        wav_file = file.replace(\"mp3\", \"\")\n",
    "        audio.export(os.path.join(tram_train_data_path, wav_file), format=\"wav\")\n",
    "\n",
    "for file in os.listdir(car_val_data_path):\n",
    "    if file.endswith(\".mp3\"):\n",
    "        audio = AudioSegment.from_mp3(os.path.join(car_val_data_path, file))\n",
    "        wav_file = file.replace(\"mp3\", \"\")\n",
    "        audio.export(os.path.join(car_val_data_path, wav_file), format=\"wav\")\n",
    "\n",
    "for file in os.listdir(tram_val_data_path):\n",
    "    if file.endswith(\".mp3\"):\n",
    "        audio = AudioSegment.from_mp3(os.path.join(tram_val_data_path, file))\n",
    "        wav_file = file.replace(\"mp3\", \"\")\n",
    "        audio.export(os.path.join(tram_val_data_path, wav_file), format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b89228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_augment(spec: np.ndarray, num_mask=2, \n",
    "                 freq_masking_max_percentage=0.10, time_masking_max_percentage=0.10):\n",
    "\n",
    "    spec = spec.copy()\n",
    "    for i in range(num_mask):\n",
    "        all_frames_num, all_freqs_num = spec.shape\n",
    "        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\n",
    "        \n",
    "        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n",
    "        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n",
    "        f0 = int(f0)\n",
    "        spec[:, f0:f0 + num_freqs_to_mask] = 0\n",
    "\n",
    "        time_percentage = random.uniform(0.0, time_masking_max_percentage)\n",
    "        \n",
    "        num_frames_to_mask = int(time_percentage * all_frames_num)\n",
    "        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n",
    "        t0 = int(t0)\n",
    "        spec[t0:t0 + num_frames_to_mask, :] = 0\n",
    "    \n",
    "    return spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "722f6d52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/juhani/Documents/Audio_Processing/Project/audio_project_work.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juhani/Documents/Audio_Processing/Project/audio_project_work.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m audio, fs \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(car_train_data_path, car), sr\u001b[39m=\u001b[39m\u001b[39m16000\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juhani/Documents/Audio_Processing/Project/audio_project_work.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#if len(audio) > longest_sample:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juhani/Documents/Audio_Processing/Project/audio_project_work.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#    longest_sample = len(audio)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juhani/Documents/Audio_Processing/Project/audio_project_work.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m audio \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((audio, np\u001b[39m.\u001b[39;49mzeros(longest_sample \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(audio))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juhani/Documents/Audio_Processing/Project/audio_project_work.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m car_audio\u001b[39m.\u001b[39mappend(audio)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juhani/Documents/Audio_Processing/Project/audio_project_work.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m mel_spec \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mmelspectrogram(y\u001b[39m=\u001b[39maudio, sr\u001b[39m=\u001b[39msample_rate, n_fft\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, hop_length\u001b[39m=\u001b[39m\u001b[39m160\u001b[39m, n_mels\u001b[39m=\u001b[39m\u001b[39m48\u001b[39m, fmax\u001b[39m=\u001b[39m\u001b[39m8000\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "car_train_data = []\n",
    "tram_train_data = []\n",
    "car_test_data = []\n",
    "tram_test_data = []\n",
    "car_val_data = []\n",
    "tram_val_data = []\n",
    "sample_rate = 16000\n",
    "car_train_data_aug = []\n",
    "tram_train_data_aug = []\n",
    "\n",
    "car_audio = []\n",
    "tram_audio = []\n",
    "#The lenght of longest audio sample is defined here.\n",
    "# For every audio sample zeros are added to make samples to be \n",
    "# in same size.\n",
    "longest_sample = 111958\n",
    "\n",
    "\n",
    "#audio = np.hstack((audio, np.zeros(longest_sample - len(audio))))\n",
    "for car in os.listdir(car_train_data_path):\n",
    "    \n",
    "    audio, fs = librosa.load(os.path.join(car_train_data_path, car), sr=16000)\n",
    "    #if len(audio) > longest_sample:\n",
    "    #    longest_sample = len(audio)\n",
    "    audio = np.hstack((audio, np.zeros(longest_sample - len(audio))))\n",
    "    car_audio.append(audio)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    car_train_data.append(mel_spec_db_norm) \n",
    "    \n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(spec_augment(mel_spec), ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    car_train_data_aug.append(mel_spec_db_norm) \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for car in os.listdir(car_val_data_path):\n",
    "    audio, fs = librosa.load(os.path.join(car_val_data_path, car), sr=16000)\n",
    "    \n",
    "    #if len(audio) > longest_sample:\n",
    "    #    longest_sample = len(audio)\n",
    "    audio = np.hstack((audio, np.zeros(longest_sample - len(audio))))\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    car_val_data.append(mel_spec_db_norm) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "for tram in os.listdir(tram_train_data_path):\n",
    "    audio, fs = librosa.load(os.path.join(tram_train_data_path, tram), sr=16000)    \n",
    "    \n",
    "    #if len(audio) > longest_sample:\n",
    "    #    longest_sample = len(audio)\n",
    "    audio = np.hstack((audio, np.zeros(longest_sample - len(audio))))\n",
    "        \n",
    "    tram_audio.append(audio)\n",
    "    \n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    tram_train_data.append(mel_spec_db_norm)\n",
    "\n",
    "    \n",
    "    mel_spec_db = librosa.power_to_db(spec_augment(mel_spec), ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    tram_train_data_aug.append(mel_spec_db_norm) \n",
    "    \n",
    "\n",
    "    \n",
    "for tram in os.listdir(tram_val_data_path):\n",
    "    audio, fs = librosa.load(os.path.join(tram_val_data_path, tram), sr=16000)\n",
    "    \n",
    "    #if len(audio) > longest_sample:\n",
    "    #    longest_sample = len(audio)\n",
    "    audio = np.hstack((audio, np.zeros(longest_sample - len(audio))))\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    tram_val_data.append(mel_spec_db_norm) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(audio): \n",
    "    return np.roll(audio,int(sample_rate/(random.randint(2,10)))) \n",
    "\n",
    "\n",
    "\n",
    "for audio_first in car_audio:\n",
    "        audio_shifted = time_shift(audio_first)*random.uniform(0.6, 0.9)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio_shifted, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "        mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "        car_train_data.append(mel_spec_db_norm) \n",
    "    \n",
    "\n",
    "for audio_first in tram_audio:\n",
    "        audio_shifted = np.add(audio_first, time_shift(audio_first)*random.uniform(0.6, 0.9))\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio_shifted, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "        mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "        tram_train_data.append(mel_spec_db_norm)\n",
    "        \n",
    "\n",
    "car_train_data = car_train_data + car_train_data_aug\n",
    "tram_train_data = tram_train_data + tram_train_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883eae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "fig, ax = plt.subplots()\n",
    "x = [\"\",\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "img = librosa.display.specshow(np.transpose(car_train_data[1]), x_axis='time',\n",
    "y_axis='mel', sr=16000, fmax=8000, ax=ax)\n",
    "\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set_xticklabels(x)\n",
    "ax.set(title='Mel-frequency Spectrogram')\n",
    "plt.savefig('mel_spec.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(car_train_data))\n",
    "print(np.shape(tram_train_data))\n",
    "print(np.shape(car_val_data))\n",
    "print(np.shape(tram_val_data))\n",
    "\n",
    "car_train_data = np.squeeze(car_train_data)\n",
    "tram_train_data = np.squeeze(tram_train_data)\n",
    "car_val_data = np.squeeze(car_val_data)\n",
    "tram_val_data = np.squeeze(tram_val_data)\n",
    "\n",
    "keyword_labels = np.ones(car_train_data.shape[0])\n",
    "val_pos_labels = np.ones(car_val_data.shape[0])\n",
    "\n",
    "noise_labels = 0*np.ones(tram_train_data.shape[0])\n",
    "val_neg_labels = 0*np.ones(tram_val_data.shape[0])\n",
    "\n",
    "X_train = np.concatenate((car_train_data, tram_train_data), axis=0)\n",
    "y_train  = np.concatenate((keyword_labels, noise_labels), axis=0)\n",
    "\n",
    "X_val = np.concatenate((car_val_data, tram_val_data), axis=0)\n",
    "y_val = np.concatenate((val_pos_labels, val_neg_labels), axis=0)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df09711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import manual_variable_initialization \n",
    "manual_variable_initialization(True)\n",
    "\n",
    "num_units = 48\n",
    "dropout_ratio = 0.1\n",
    "checkpoint_filepath = f'D:/Koulujutut/audio_project/keyword_model_1D_CRNN'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, mode='min'),\n",
    "                tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, \n",
    "                monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(car_train_data.shape[1], car_train_data.shape[2])),\n",
    "    Conv1D(num_units,3,padding=\"same\",activation='relu',name='layer1'),\n",
    "    Dropout(dropout_ratio),\n",
    "    Conv1D(num_units,3,padding=\"same\",activation='relu',name='layer2'),\n",
    "    Dropout(dropout_ratio),\n",
    "    GRU(num_units,name='RNN_1',return_sequences=False),\n",
    "    Dense(2, activation='softmax', name='dense_a'),\n",
    "])\n",
    "\n",
    "model.compile(loss='BinaryCrossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=32, callbacks=my_callbacks, \n",
    "                    validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = tf.keras.Model()\n",
    "#model.load_model(latest)\n",
    "model=tf.keras.models.load_model('D:/Koulujutut/audio_project/keyword_model_1D_CRNN')\n",
    "\n",
    "model.evaluate(X_val, y_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_test_data_path = f\"D:/Koulujutut/audio_project/test_data/car/\"\n",
    "tram_test_data_path = f\"D:/Koulujutut/audio_project/test_data/tram/\"\n",
    "car_test_data = []\n",
    "tram_test_data = []\n",
    "\n",
    "for file in os.listdir(car_test_data_path):\n",
    "    if file.endswith(\".mp3\"):\n",
    "        audio = AudioSegment.from_mp3(os.path.join(car_test_data_path, file))\n",
    "        wav_file = file.replace(\".mp3\", \"\")\n",
    "        audio.export(os.path.join(car_test_data_path, wav_file), format=\"wav\")\n",
    "\n",
    "for file in os.listdir(tram_test_data_path):\n",
    "    if file.endswith(\".mp3\"):\n",
    "        audio = AudioSegment.from_mp3(os.path.join(tram_test_data_path, file))\n",
    "        wav_file = file.replace(\".mp3\", \"\")\n",
    "        audio.export(os.path.join(tram_test_data_path, wav_file), format=\"wav\")\n",
    "\n",
    "for file in os.listdir(car_test_data_path):\n",
    "    audio, fs = librosa.load(os.path.join(car_test_data_path, file), sr=16000)\n",
    "    #if len(audio) > longest_sample:\n",
    "    #    longest_sample = len(audio)\n",
    "    audio = np.hstack((audio, np.zeros(longest_sample - len(audio))))\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    car_test_data.append(mel_spec_db_norm) \n",
    "\n",
    "\n",
    "for file in os.listdir(tram_test_data_path):\n",
    "    audio, fs = librosa.load(os.path.join(tram_test_data_path, file), sr=16000)\n",
    "    #if len(audio) > longest_sample:\n",
    "    #    longest_sample = len(audio)\n",
    "    audio = np.hstack((audio, np.zeros(longest_sample - len(audio))))\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=512, hop_length=160, n_mels=48, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).T\n",
    "    mel_spec_db_norm = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "    tram_test_data.append(mel_spec_db_norm) \n",
    "\n",
    "#print(longest_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cab00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(car_test_data))\n",
    "print(np.shape(tram_test_data))\n",
    "car_test_data = np.squeeze(car_test_data)\n",
    "tram_test_data = np.squeeze(tram_test_data)\n",
    "test_pos_labels = np.ones(car_test_data.shape[0])\n",
    "test_neg_labels = 0*np.ones(tram_test_data.shape[0])\n",
    "\n",
    "X_test = np.concatenate((car_test_data, tram_test_data), axis=0)\n",
    "y_test = np.concatenate((test_pos_labels, test_neg_labels), axis=0)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.evaluate(X_test, y_test, batch_size=32)\n",
    "hypothesis= model.predict(X_test, batch_size=32)\n",
    "y_pred = tf.keras.utils.to_categorical(hypothesis, 2)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_true = y_test[:, 1]\n",
    "#fpr, tpr, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "\n",
    "y_pred = np.round(hypothesis[:, 1])\n",
    "CM = confusion_matrix(y_true, y_pred)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "Precision = TP/(TP+FP)\n",
    "Recall = TP/(TP+FN)\n",
    "F1_score = 2*((Precision*Recall)/(Precision+Recall))\n",
    "print(f\"true negatives: {TN}, false negatives: {FN}, true positives {TP}, false positives {FP}\")\n",
    "print(f\" Precision: {Precision}\")\n",
    "print(f\" Recall: {Recall}\")\n",
    "print(f\" F1 score: {F1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0cf239eacaabca9aef8a58b98060c3a682f2d2c3c539471177ff69c6a52bdd3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
